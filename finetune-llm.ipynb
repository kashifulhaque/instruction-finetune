{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7419775,"sourceType":"datasetVersion","datasetId":4316700}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Fine-tune LLM on custom dataset**\n\nArticle: [part 1](https://wandb.ai/capecape/alpaca_ft/reports/How-to-implement-fine-tuning-of-an-LLM-Part-1-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2) [part 2](https://wandb.ai/capecape/alpaca_ft/reports/How-to-fine-tune-an-LLM-Part-2-Instruction-tuning-Llama-2--Vmlldzo1NjY0MjE1) \\\nCleaned dataset in use: [alpaca cleaned](https://github.com/gururise/AlpacaDataCleaned/blob/main/alpaca_data_cleaned.json)","metadata":{}},{"cell_type":"markdown","source":"## **Dependencies**","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.104383Z","iopub.execute_input":"2024-01-19T10:43:12.104788Z","iopub.status.idle":"2024-01-19T10:43:12.109319Z","shell.execute_reply.started":"2024-01-19T10:43:12.104757Z","shell.execute_reply":"2024-01-19T10:43:12.108231Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"### **Load dataset**","metadata":{}},{"cell_type":"code","source":"import json\n\nfrom pprint import pprint","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.113340Z","iopub.execute_input":"2024-01-19T10:43:12.114091Z","iopub.status.idle":"2024-01-19T10:43:12.121373Z","shell.execute_reply.started":"2024-01-19T10:43:12.114062Z","shell.execute_reply":"2024-01-19T10:43:12.120515Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/alpaca-cleaned/alpaca_data_cleaned.json\", \"r\") as file:\n    alpaca = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.122691Z","iopub.execute_input":"2024-01-19T10:43:12.122985Z","iopub.status.idle":"2024-01-19T10:43:12.394458Z","shell.execute_reply.started":"2024-01-19T10:43:12.122961Z","shell.execute_reply":"2024-01-19T10:43:12.393485Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"len(alpaca)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.397062Z","iopub.execute_input":"2024-01-19T10:43:12.397340Z","iopub.status.idle":"2024-01-19T10:43:12.403596Z","shell.execute_reply.started":"2024-01-19T10:43:12.397318Z","shell.execute_reply":"2024-01-19T10:43:12.402689Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"51760"},"metadata":{}}]},{"cell_type":"code","source":"pprint(alpaca[123])","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.404861Z","iopub.execute_input":"2024-01-19T10:43:12.405197Z","iopub.status.idle":"2024-01-19T10:43:12.413733Z","shell.execute_reply.started":"2024-01-19T10:43:12.405162Z","shell.execute_reply":"2024-01-19T10:43:12.412606Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"{'input': '',\n 'instruction': \"Find the synonyms of the following word: 'Tenacious'.\",\n 'output': \"Here are some synonyms for the word 'Tenacious':\\n\"\n           '\\n'\n           '1. Persistent\\n'\n           '2. Determined \\n'\n           '3. Resolute \\n'\n           '4. Steadfast \\n'\n           '5. Obstinate\\n'\n           '6. Persevering\\n'\n           '7. Unyielding\\n'\n           '8. Unwavering\\n'\n           '9. Strong-willed\\n'\n           '10. Dogged.'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Preprocess the data**\nAs we have instructions both with and without prompts, hence each case must be dealt with them separately.","metadata":{}},{"cell_type":"code","source":"def prompt_no_input(row):\n    return (\"Below is an instruction that describes a task. \"\n            \"Write a response that appropriately completes the request.\\n\\n\"\n            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.416035Z","iopub.execute_input":"2024-01-19T10:43:12.416695Z","iopub.status.idle":"2024-01-19T10:43:12.423344Z","shell.execute_reply.started":"2024-01-19T10:43:12.416662Z","shell.execute_reply":"2024-01-19T10:43:12.422566Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"def prompt_input(row):\n    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n            \"Write a response that appropriately completes the request.\\n\\n\"\n            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.424318Z","iopub.execute_input":"2024-01-19T10:43:12.424553Z","iopub.status.idle":"2024-01-19T10:43:12.432339Z","shell.execute_reply.started":"2024-01-19T10:43:12.424532Z","shell.execute_reply":"2024-01-19T10:43:12.431443Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"print(prompt_no_input(alpaca[123]))","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.433416Z","iopub.execute_input":"2024-01-19T10:43:12.433773Z","iopub.status.idle":"2024-01-19T10:43:12.442832Z","shell.execute_reply.started":"2024-01-19T10:43:12.433741Z","shell.execute_reply":"2024-01-19T10:43:12.441986Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nFind the synonyms of the following word: 'Tenacious'.\n\n### Response:\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **We can merge both paths into one**","metadata":{}},{"cell_type":"code","source":"def create_prompt(row):\n    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.443931Z","iopub.execute_input":"2024-01-19T10:43:12.444260Z","iopub.status.idle":"2024-01-19T10:43:12.451265Z","shell.execute_reply.started":"2024-01-19T10:43:12.444235Z","shell.execute_reply":"2024-01-19T10:43:12.450418Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"prompts = [create_prompt(row) for row in alpaca]","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.452302Z","iopub.execute_input":"2024-01-19T10:43:12.452599Z","iopub.status.idle":"2024-01-19T10:43:12.533298Z","shell.execute_reply.started":"2024-01-19T10:43:12.452575Z","shell.execute_reply":"2024-01-19T10:43:12.532650Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"### **End-of-String tokens (EOS)**\nThis token is essential because it tells the model when to stop producing text \\\nFor LLaMa models, it is `EOS_TOKEN = \"</s>\"`","metadata":{}},{"cell_type":"code","source":"# Append EOS after each response\nEOS_TOKEN = \"</s>\"\noutputs = [row[\"output\"] + EOS_TOKEN for row in alpaca]","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.534282Z","iopub.execute_input":"2024-01-19T10:43:12.534515Z","iopub.status.idle":"2024-01-19T10:43:12.570303Z","shell.execute_reply.started":"2024-01-19T10:43:12.534495Z","shell.execute_reply":"2024-01-19T10:43:12.569650Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"print(outputs[0])","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.574585Z","iopub.execute_input":"2024-01-19T10:43:12.574916Z","iopub.status.idle":"2024-01-19T10:43:12.579350Z","shell.execute_reply.started":"2024-01-19T10:43:12.574893Z","shell.execute_reply":"2024-01-19T10:43:12.578405Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.</s>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Concatenate instructions and outputs to form dataset**","metadata":{}},{"cell_type":"code","source":"dataset = [{\n    \"prompt\": s,\n    \"output\": t,\n    \"example\": s + t\n} for s, t in zip(prompts, outputs)]","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.580481Z","iopub.execute_input":"2024-01-19T10:43:12.580828Z","iopub.status.idle":"2024-01-19T10:43:12.639667Z","shell.execute_reply.started":"2024-01-19T10:43:12.580798Z","shell.execute_reply":"2024-01-19T10:43:12.638931Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"### **Time to tokenize**\nWe need to convert the dataset into tokens.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.640549Z","iopub.execute_input":"2024-01-19T10:43:12.640824Z","iopub.status.idle":"2024-01-19T10:43:12.644815Z","shell.execute_reply.started":"2024-01-19T10:43:12.640801Z","shell.execute_reply":"2024-01-19T10:43:12.643876Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"model_id = \"mistralai/Mistral-7B-v0.1\"\n# model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:12.645975Z","iopub.execute_input":"2024-01-19T10:43:12.646546Z","iopub.status.idle":"2024-01-19T10:43:14.167460Z","shell.execute_reply.started":"2024-01-19T10:43:12.646514Z","shell.execute_reply":"2024-01-19T10:43:14.166483Z"},"trusted":true},"execution_count":86,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b249cede5e6042ed963dd10388c40d32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d576433715e4a8aaf8d0648fdc14ad7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25461cf42eb54079a9562371b1208237"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33ba135e0b2742c7b60aec5e9d0c11fd"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Sample tokens**","metadata":{}},{"cell_type":"code","source":"tokenizer.encode(\n    \"This sentence is sentenced for tokenization!\",\n    padding = \"max_length\",\n    max_length = 10,\n    return_tensors = \"pt\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:14.168659Z","iopub.execute_input":"2024-01-19T10:43:14.168941Z","iopub.status.idle":"2024-01-19T10:43:14.176533Z","shell.execute_reply.started":"2024-01-19T10:43:14.168917Z","shell.execute_reply":"2024-01-19T10:43:14.175749Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"tensor([[    1,   851, 12271,   349,  2662,  4697,   354,  6029,  1837, 28808]])"},"metadata":{}}]},{"cell_type":"markdown","source":"### **Creating a train-eval split**","metadata":{}},{"cell_type":"code","source":"import random\n\n# shuffle in-place\nrandom.shuffle(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:14.177662Z","iopub.execute_input":"2024-01-19T10:43:14.177934Z","iopub.status.idle":"2024-01-19T10:43:14.230819Z","shell.execute_reply.started":"2024-01-19T10:43:14.177910Z","shell.execute_reply":"2024-01-19T10:43:14.230133Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"train_dataset = dataset[:-1000]\neval_dataset = dataset[-1000:]\n\ntrain_table = pd.DataFrame(train_dataset)\neval_table = pd.DataFrame(eval_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:14.231798Z","iopub.execute_input":"2024-01-19T10:43:14.232063Z","iopub.status.idle":"2024-01-19T10:43:14.344182Z","shell.execute_reply.started":"2024-01-19T10:43:14.232039Z","shell.execute_reply":"2024-01-19T10:43:14.343431Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"### **Packing: Combine multiple samples into a longer sequence**\n> To make training more efficient and use the longer context of these LLMs we'll do something called **\"packing\"** \\\nWe will combine multiple examples to fill the model's memory and make training more efficient instead of feeding examples individually.\n\nThe main idea here is that the instruction/output samples are short, so let's concatenate a bunch of them together, separated by the EOS token.","metadata":{}},{"cell_type":"code","source":"max_seq_len = 1024","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:14.345716Z","iopub.execute_input":"2024-01-19T10:43:14.346066Z","iopub.status.idle":"2024-01-19T10:43:14.350437Z","shell.execute_reply.started":"2024-01-19T10:43:14.346033Z","shell.execute_reply":"2024-01-19T10:43:14.349472Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"def pack(dataset, max_seq_len = 1024):\n    tkds_ids = tokenizer([s[\"example\"] for s in dataset])[\"input_ids\"]\n    all_token_ids = []\n    packed_ds = []\n    \n    for tokenized_input in tkds_ids:\n        all_token_ids.extend(tokenized_input + [tokenizer.eos_token_id])\n\n    for i in range(0, len(all_token_ids), max_seq_len+1):\n        input_ids = all_token_ids[i : i + max_seq_len+1]\n        \n        if len(input_ids) == (max_seq_len+1):\n            packed_ds.append({ \"input_ids\": input_ids[:-1], \"labels\": input_ids[1:] })\n\n    return packed_ds","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:14.351600Z","iopub.execute_input":"2024-01-19T10:43:14.352004Z","iopub.status.idle":"2024-01-19T10:43:14.360595Z","shell.execute_reply.started":"2024-01-19T10:43:14.351973Z","shell.execute_reply":"2024-01-19T10:43:14.359794Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"train_ds_packed = pack(train_dataset)\neval_ds_packed = pack(eval_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:14.361783Z","iopub.execute_input":"2024-01-19T10:43:14.362044Z","iopub.status.idle":"2024-01-19T10:43:28.475933Z","shell.execute_reply.started":"2024-01-19T10:43:14.362021Z","shell.execute_reply":"2024-01-19T10:43:28.475121Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"## **Storing our preprocessed datasets**","metadata":{}},{"cell_type":"code","source":"def save_jsonl(data, filename):\n    with open(filename, \"w\") as file:\n        for entry in data:\n            json.dump(entry, file)\n            file.write(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:28.477020Z","iopub.execute_input":"2024-01-19T10:43:28.477308Z","iopub.status.idle":"2024-01-19T10:43:28.482372Z","shell.execute_reply.started":"2024-01-19T10:43:28.477283Z","shell.execute_reply":"2024-01-19T10:43:28.481456Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"save_jsonl(train_ds_packed, \"train_packed_alpaca.jsonl\")\nsave_jsonl(eval_ds_packed, \"eval_packed_alpaca.jsonl\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:28.483516Z","iopub.execute_input":"2024-01-19T10:43:28.483788Z","iopub.status.idle":"2024-01-19T10:43:49.047113Z","shell.execute_reply.started":"2024-01-19T10:43:28.483764Z","shell.execute_reply":"2024-01-19T10:43:49.046218Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"# **Loading the preprocessed dataset**","metadata":{}},{"cell_type":"code","source":"import json","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:49.048203Z","iopub.execute_input":"2024-01-19T10:43:49.048494Z","iopub.status.idle":"2024-01-19T10:43:49.052534Z","shell.execute_reply.started":"2024-01-19T10:43:49.048469Z","shell.execute_reply":"2024-01-19T10:43:49.051624Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"def load_jsonl(filename):\n    data = []\n    \n    with open(filename, \"r\") as file:\n        for line in file:\n            data.append(json.loads(line))\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:49.053642Z","iopub.execute_input":"2024-01-19T10:43:49.053956Z","iopub.status.idle":"2024-01-19T10:43:49.063049Z","shell.execute_reply.started":"2024-01-19T10:43:49.053930Z","shell.execute_reply":"2024-01-19T10:43:49.062086Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"train_ds_packed = load_jsonl(\"/kaggle/working/train_packed_alpaca.jsonl\")\neval_ds_packed = load_jsonl(\"/kaggle/working/eval_packed_alpaca.jsonl\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:49.064195Z","iopub.execute_input":"2024-01-19T10:43:49.064816Z","iopub.status.idle":"2024-01-19T10:43:52.901469Z","shell.execute_reply.started":"2024-01-19T10:43:49.064761Z","shell.execute_reply":"2024-01-19T10:43:52.900489Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"## **Data Loader**","metadata":{}},{"cell_type":"markdown","source":"A standard PyTorch dataloader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import default_data_collator","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:52.902857Z","iopub.execute_input":"2024-01-19T10:43:52.903250Z","iopub.status.idle":"2024-01-19T10:43:52.908046Z","shell.execute_reply.started":"2024-01-19T10:43:52.903216Z","shell.execute_reply":"2024-01-19T10:43:52.907170Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"batch_size = 8","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:52.909115Z","iopub.execute_input":"2024-01-19T10:43:52.909391Z","iopub.status.idle":"2024-01-19T10:43:52.919833Z","shell.execute_reply.started":"2024-01-19T10:43:52.909368Z","shell.execute_reply":"2024-01-19T10:43:52.919012Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(\n    train_ds_packed,\n    batch_size = batch_size,\n    collate_fn = default_data_collator\n)\n\neval_dataloader = DataLoader(\n    eval_ds_packed,\n    batch_size = batch_size,\n    collate_fn = default_data_collator,\n    shuffle = False\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:52.920921Z","iopub.execute_input":"2024-01-19T10:43:52.921296Z","iopub.status.idle":"2024-01-19T10:43:53.166870Z","shell.execute_reply.started":"2024-01-19T10:43:52.921263Z","shell.execute_reply":"2024-01-19T10:43:53.165823Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"b = next(iter(train_dataloader))\nb.keys(), b[\"input_ids\"][0][:25], b[\"labels\"][0][:25]","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:53.175846Z","iopub.execute_input":"2024-01-19T10:43:53.176132Z","iopub.status.idle":"2024-01-19T10:43:53.190407Z","shell.execute_reply.started":"2024-01-19T10:43:53.176108Z","shell.execute_reply":"2024-01-19T10:43:53.189607Z"},"trusted":true},"execution_count":101,"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"(dict_keys(['input_ids', 'labels']),\n tensor([    1, 20811,   349,   396, 13126,   369, 13966,   264,  3638, 28725,\n          5881,  1360,   395,   396,  2787,   369,  5312,  3629,  2758, 28723,\n         12018,   264,  2899,   369,  6582]),\n tensor([20811,   349,   396, 13126,   369, 13966,   264,  3638, 28725,  5881,\n          1360,   395,   396,  2787,   369,  5312,  3629,  2758, 28723, 12018,\n           264,  2899,   369,  6582,  1999]))"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Training Loop**\nWe'll train the model and make the model complete the sentence naively.\n\nWe'll also be using `SimpleNamespace` to access attributes with a dot `.` like `config.batch_size` and not `config[\"batch_size\"]`","metadata":{}},{"cell_type":"code","source":"from types import SimpleNamespace","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:53.191794Z","iopub.execute_input":"2024-01-19T10:43:53.192175Z","iopub.status.idle":"2024-01-19T10:43:53.197319Z","shell.execute_reply.started":"2024-01-19T10:43:53.192143Z","shell.execute_reply":"2024-01-19T10:43:53.196473Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"config = SimpleNamespace(\n    model_id = model_id,\n    dataset_name = \"alpaca-cleaned\",\n    precision = \"bf16\",\n    n_freeze = 24,        # number of layers we dont train LLaMa 7B has 32\n    lr = 2e-4,\n    n_eval_sample = 10,   # number of samples to generate on validation\n    max_seq_len = 1024,   # Length of the sequences to pack\n    epochs = 3,\n    gradient_accumulation_steps = 32 // batch_size,  # how many iterations we update the gradients\n    batch_size = batch_size,\n    log_model = False,\n    mom = 0.9,         # momentum\n    gradient_checkpointing = True,\n    freeze_embed = True\n)\n\nconfig.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:53.198391Z","iopub.execute_input":"2024-01-19T10:43:53.198750Z","iopub.status.idle":"2024-01-19T10:43:53.207482Z","shell.execute_reply.started":"2024-01-19T10:43:53.198709Z","shell.execute_reply":"2024-01-19T10:43:53.206651Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"**Get a pre-trained model with some config parameters**","metadata":{}},{"cell_type":"code","source":"import torch\n\nfrom transformers import AutoModelForCausalLM","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:53.208672Z","iopub.execute_input":"2024-01-19T10:43:53.209074Z","iopub.status.idle":"2024-01-19T10:43:53.219211Z","shell.execute_reply.started":"2024-01-19T10:43:53.209044Z","shell.execute_reply":"2024-01-19T10:43:53.218469Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    config.model_id,\n    device_map = 0,\n    trust_remote_code = True,\n    low_cpu_mem_usage = True,\n    torch_dtype = torch.float16,\n    use_cache = False\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:43:53.220269Z","iopub.execute_input":"2024-01-19T10:43:53.220826Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6a197d41b044a1aa69ab69dba380510"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f60ba66742e40f4a21b846699f44c95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a59ba78b17f94f35ae7b22b23b4b5ebe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22157432e1714a72aa167823e73176c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beb6932df9ce4e1a8f70dff94f69c75d"}},"metadata":{}}]},{"cell_type":"markdown","source":"## **Freezing the Model to Save Memory**\n> Training the full models is expensive\n\nInstead, we will train a subset of the model parameters. This technique was pioneered by [Jeremy Howard and Seb Ruder](<https://arxiv.org/abs/1801.06146>)\n\n**Transformer-based models like Llama are a stack of identical layers on top of each other with a classification layer at the end.** LLaMa 2 (7B) has 32 transformer layers, we will only train the last 8 layers (Number of layers to freeze can be experimented). You always want to train the classification head (the last layer which makes predictions)\n\n![](https://storage.googleapis.com/wandb-production.appspot.com/capecape/images/projects/38233410/0a3d44f0.png)","metadata":{}},{"cell_type":"markdown","source":"Before we do any fancy parameter efficient methods, let's freeze most model layers. After loading the model, we freeze most of it. This way, we save a ton of memory by not computing gradients on the frozen layers.","metadata":{}},{"cell_type":"code","source":"n_freeze = 24","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Freeze gradients (disable gradients)**","metadata":{}},{"cell_type":"code","source":"for param in model.parameters(): param.requires_grad = False\nfor param in model.lm_head.parameters(): param.requires_grad = False\nfor param in model.model.layers[n_freeze:].parameters(): param.requires_grad = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**can even gain a little bit more memory by freezing the embeddings!**","metadata":{}},{"cell_type":"code","source":"if config.freeze_embed:\n    model.model.embed_tokens.requires_grad_(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**can also use gradient checkpointing to save even more memory (makes training slower)**","metadata":{}},{"cell_type":"code","source":"if config.gradient_checkpointing:\n    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs = {\n        \"use_reentrant\": False\n    })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Optimizer and Scheduler**\n`Adam` and `cosine_schedule` are safe starting points. We'll make use of our training loop using `bfloat` to make good use of those tensor cores available on modern nvidia gpus. Cross entropy will be the loss function","metadata":{}},{"cell_type":"code","source":"from transformers import get_cosine_schedule_with_warmup","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optim = torch.optim.Adam(\n    model.parameters(),\n    lr = config.lr,\n    betas = (0.9, 0.99),\n    eps = 1e-5\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scheduler = get_cosine_schedule_with_warmup(\n    optim,\n    num_training_steps = config.total_train_steps,\n    num_warmup_steps = config.total_train_steps // 10\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_fn(x, y):\n    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Sampling from the model**\nMake a simple function to sample from the model now and then to visually see what the model is outputting","metadata":{}},{"cell_type":"code","source":"from transformers import GenerationConfig","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_config = GenerationConfig.from_pretrained(config.model_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate(prompt, max_new_tokens = 100, gen_config = gen_config):\n    with torch.inference_mode():\n        tokenized_prompt = tokenizer(prompt, return_tensors = \"pt\")[\"input_ids\"].cuda()\n        \n        output = model.generate(\n            tokenized_prompt,\n            max_new_tokens = max_new_tokens,\n            generation_config = gen_config\n        )\n    \n    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = eval_dataset[14][\"prompt\"]\nprint(prompt + generate(prompt, 128))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Accuracy:\n    def __init__(self):\n        self.count = 0\n        self.tp = 0.\n    \n    def update(self, logits, labels):\n        logits, labels = logits.argmax(dim = -1).view(-1).cpu(), labels.view(-1).cpu()\n        tp = (logits == labels).sum()\n        self.count += len(logits)\n        self.tp += tp\n        \n        return tp / len(logits)\n    \n    def compute(self):\n        return self.tp / self.count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_gpu(tensor_dict):\n    return { k: v.to('cuda') for k, v in tensor_dict.items() }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Save the model checkpoints**","metadata":{}},{"cell_type":"code","source":"from pathlib import Path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model(model, model_name, models_folder = \"/kaggle/working\"):\n    model_name = f\"{model_name}\"\n    file_name = Path(f\"{models_folder}/{model_name}\")\n    file_name.parent.mkdir(parents = True, exist_ok = True)\n    model.save_pretrained(file_name, safe_serialization = True)\n    \n    # save tokenizer for easy inference\n    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n    tokenizer.save_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Training**","metadata":{}},{"cell_type":"code","source":"from tqdm.auto import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = Accuracy()\nmodel.train()\ntrain_step = 0\npbar = tqdm(total = config.total_train_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1):\n    for step, batch in enumerate(train_dataloader):\n        batch = to_gpu(batch)\n        \n        with torch.amp.autocast(\"cuda\", dtype = torch.float16):\n            out = model(**batch)\n            loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps\n            loss.backward()\n        \n        if step % config.gradient_accumulation_steps == 0:\n            optim.step()\n            scheduler.step()\n            optim.zero_grad(set_to_none = True)\n            train_step += 1\n            pbar.update(1)\n            \npbar.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_model(model, model_name = config.model_id.replace(\"/\", \"_\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}